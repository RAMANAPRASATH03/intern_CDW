{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "text = \"\"\"\n",
    "\n",
    "English and CAPITALIZATION\n",
    "\n",
    "ðŸŽµé¸Ÿ\n",
    "show_tokens False None elif == >= else: two tabs:\" \" Three tabs: \"   \"\n",
    "\n",
    "12.0*50=600\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "colors_list = [\n",
    "    '102;194;165', '252;141;98', '141;160;203',\n",
    "    '231;138;195', '166;216;84', '255;217;47'\n",
    "]\n",
    "\n",
    "def show_tokens(sentence, tokenizer_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    token_ids = tokenizer(sentence).input_ids\n",
    "    for idx, t in enumerate(token_ids):\n",
    "        print(\n",
    "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
    "            tokenizer.decode(t) +\n",
    "            '\\x1b[0m',\n",
    "            end=' '\n",
    "        )\n",
    "\n",
    "show_tokens(text, 'bert-base-uncased')\n",
    "\n",
    "#WordPiece\n",
    "\n",
    "show_tokens(text, 'bert-base-cased')\n",
    "#WordPiece\n",
    "\n",
    "show_tokens(text, 'gpt2')\n",
    "#BPE\n",
    "\n",
    "show_tokens(text, 'google/flan-t5-xxl')\n",
    "#SentencePiece\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers LM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'  # You can choose other BERT models\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Or use a sentence transformer model directly for better performance\n",
    "# model = SentenceTransformer('all-mpnet-base-v2') # Example: all-mpnet-base-v2\n",
    "\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Average pooling of token embeddings\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    embedding1 = get_bert_embedding(text1)\n",
    "    embedding2 = get_bert_embedding(text2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = util.cos_sim(embedding1, embedding2).item()\n",
    "    return similarity_score\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text1 = \"press and hold SCROLL Hand eraser\"\n",
    "text2 = \"conflict is the compramise that made by DEVILs DiaviLLE\"\n",
    "text3 = \"will one of my eye swells out am i able to see\"\n",
    "\n",
    "similarity_1_2 = calculate_similarity(text1, text2)\n",
    "similarity_1_3 = calculate_similarity(text1, text3)\n",
    "\n",
    "\n",
    "print(f\"Similarity between '{text1}' and '{text2}': {similarity_1_2}\")\n",
    "print(f\"Similarity between '{text1}' and '{text3}': {similarity_1_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day-4 exercise using hugging face models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers\n",
    "\n",
    "# Import the datasets and transformers packages\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the train and test splits of the imdb dataset\n",
    "splits = [\"train\", \"test\"]\n",
    "ds = {split: ds for split, ds in zip(splits, load_dataset(\"imdb\", split=splits))}\n",
    "\n",
    "# Thin out the dataset to make it run faster for this example\n",
    "for split in splits:\n",
    "    ds[split] = ds[split].shuffle(seed=42).select(range(500))\n",
    "\n",
    "# Show the dataset\n",
    "print(ds)\n",
    "\n",
    "# Load the tokenizer for the \"bert-base-uncased\" model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Preprocess function for tokenization\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the imdb dataset by returning tokenized examples.\"\"\"\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_ds = {}\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = ds[split].map(preprocess_function, batched=True)\n",
    "\n",
    "# Check that we tokenized the examples properly\n",
    "assert tokenized_ds[\"train\"][0][\"input_ids\"][:5] == [101, 2045, 2003, 2053, 7189]\n",
    "\n",
    "# Show the first example of the tokenized training set\n",
    "print(tokenized_ds[\"train\"][0][\"input_ids\"])\n",
    "\n",
    "# Load the model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},  # For converting predictions to strings\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    ")\n",
    "\n",
    "# Freeze all the parameters of the base model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Check the classifier head\n",
    "print(model.classifier)\n",
    "\n",
    "# Print the entire model architecture\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
