{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Day 5 class session\n",
    "Embeddings\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load a language model\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "##microsoft/deberta-v3-xsmall\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer('Hello world', return_tensors='pt')\n",
    "\n",
    "# Process the tokens\n",
    "output = model(**tokens)[0]\n",
    "\n",
    "output.shape\n",
    "\n",
    "for token in tokens['input_ids'][0]:\n",
    "    print(tokenizer.decode(token))\n",
    "\n",
    "output\n",
    "\n",
    "### Scaled Dot Product Attention\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "len(model.encoder.layer)\n",
    "\n",
    "model.encoder.layer[0]\n",
    "\n",
    "model.encoder.layer[0].attention\n",
    "\n",
    "## Multi Headed Attention\n",
    "\n",
    "from transformers import BertConfig\n",
    "config = BertConfig()\n",
    "config\n",
    "\n",
    "!pip install bertviz\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from bertviz import head_view\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"My friend told me about this series and I loved it so far! She was Right\"\n",
    "\n",
    "token = tokenizer.encode(text)\n",
    "\n",
    "input = torch.tensor(token).unsqueeze(0)\n",
    "\n",
    "input\n",
    "\n",
    "attention = model(input, output_attentions=True)[2] #Extract Attention Weights from BERT model\n",
    "\n",
    "attention\n",
    "\n",
    "#average attention in all heads\n",
    "final_attention = attention[-1].mean(1)[0]\n",
    "final_attention.size()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "attention_df = pd.DataFrame(final_attention.detach()).applymap(float).round(3)\n",
    "\n",
    "attention_df.columns = tokenizer.convert_ids_to_tokens(token)\n",
    "attention_df.index = tokenizer.convert_ids_to_tokens(token)\n",
    "\n",
    "attention_df\n",
    "\n",
    "tokens_as_list = tokenizer.convert_ids_to_tokens(input[0])\n",
    "head_view(attention, tokens_as_list)\n",
    "\n",
    "#2 - Previous & 6 - Pronouns\n",
    "\n",
    "#previous tokens\n",
    "head_view(attention,tokenizer.convert_ids_to_tokens(input[0]),layer=2, heads=[0])\n",
    "\n",
    "#object - verb\n",
    "head_view(attention,tokenizer.convert_ids_to_tokens(input[0]),layer=7, heads=[9])\n",
    "\n",
    "## Day 4 afternoon session\n",
    "\n",
    "!pip install bertviz\n",
    "\n",
    "from transformers import pipeline, set_seed, AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "from torch import tensor, numel\n",
    "from bertviz import model_view, head_view\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# pipeline object in transformers provides easy access to transformer usage\n",
    "MODEL = 'gpt2'\n",
    "\n",
    "generator = pipeline('text-generation', model=MODEL)\n",
    "\n",
    "# finish the sentence\n",
    "generator(\"Hello, I'm a language model and I\", max_length=30, num_return_sequences=3)\n",
    "\n",
    "# load up a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "'Sinan' in tokenizer.get_vocab()\n",
    "\n",
    "tokenizer.encode('Sinan loves a beautiful day')\n",
    "\n",
    "# encode a string and then convert the ids back into tokens. Note the Ä  character denoting a space before the token\n",
    "tokenizer.convert_ids_to_tokens(tokenizer.encode('Sinan loves a beautiful day'))\n",
    "\n",
    "tokenizer.encode('Sinan loves a beautiful day')  # ids\n",
    "\n",
    "encoded = tokenizer.encode('Sinan loves a beautiful day', return_tensors='pt')  # as a pytorch tensor\n",
    "\n",
    "encoded\n",
    "\n",
    "# load up a tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
    "\n",
    "model\n",
    "\n",
    "encoded\n",
    "\n",
    "model.transformer.wte(encoded)\n",
    "\n",
    "model.transformer.wte(encoded).shape  # 1 item in batch x 6 tokens x token dimension\n",
    "\n",
    "model.transformer.wpe(tensor([0, 1, 2, 3, 4, 5]).reshape(1, 6)).shape  # manually create position vectors\n",
    "\n",
    "# create GPT input\n",
    "initial_input = model.transformer.wte(encoded) + model.transformer.wpe(tensor([0, 1, 2, 3, 4, 5]).reshape(1, 6))\n",
    "\n",
    "initial_input.shape\n",
    "\n",
    "initial_input = model.transformer.drop(initial_input)  # run our input through the model's initual dropout later\n",
    "initial_input\n",
    "\n",
    "model.lm_head\n",
    "\n",
    "for module in model.transformer.h:  # run the initial_input through every decoder in the stack\n",
    "    initial_input = module(initial_input)[0]\n",
    "\n",
    "initial_input = model.transformer.ln_f(initial_input)  # and then the final layer norm\n",
    "\n",
    "initial_input\n",
    "\n",
    "# same as just running through the model\n",
    "(initial_input == model(encoded, output_hidden_states=True).hidden_states[-1]).all()\n",
    "\n",
    "model(encoded).logits.shape\n",
    "\n",
    "total_params = 0\n",
    "for param in model.parameters():\n",
    "    total_params += numel(param)\n",
    "\n",
    "print(f'Number of params: {total_params:,}')\n",
    "\n",
    "## Masked multi-headed attention**\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "phrase = 'My friend was right about this class. It is so fun!'\n",
    "encoded_phrase = tokenizer(phrase, return_tensors='pt')\n",
    "\n",
    "response = model(**encoded_phrase, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "len(response.attentions)\n",
    "\n",
    "encoded_phrase\n",
    "\n",
    "\n",
    "response.attentions[-1].shape  # represtnations from the final decoder\n",
    "\n",
    "encoded_phrase['input_ids'].shape\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_phrase['input_ids'][0])\n",
    "\n",
    "tokens\n",
    "\n",
    "# Layer index 9, head 0. Check out the almost 60% attention the token it is giving to the token class\n",
    "arr = response.attentions[9][0][0]\n",
    "\n",
    "n_digits = 3\n",
    "\n",
    "attention_df = pd.DataFrame((torch.round(arr * 10**n_digits) / (10**n_digits)).detach()).applymap(float)\n",
    "\n",
    "attention_df.columns = tokens\n",
    "attention_df.index = tokens\n",
    "\n",
    "attention_df\n",
    "\n",
    "head_view(response.attentions, tokens)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_phrase['input_ids'][0])\n",
    "model_view(response.attentions, tokens)\n",
    "\n",
    "response.hidden_states[-1].shape\n",
    "\n",
    "response.logits\n",
    "\n",
    "response.logits.shape\n",
    "\n",
    "# look at the top next token in the auto-regressive language modelling task\n",
    "pd.DataFrame(\n",
    "    zip(tokens, tokenizer.convert_ids_to_tokens(response.logits.argmax(2)[0])),\n",
    "    columns=['Sequence up until', 'Next token with highest probability']\n",
    ")\n",
    "\n",
    "generator('My friend was right', max_length=12, num_return_sequences=5)\n",
    "\n",
    "generator(phrase, max_length=20, num_return_sequences=1, do_sample=False)  # greedy search\n",
    "\n",
    "generator(phrase, max_length=20, num_return_sequences=1, do_sample=True)  # greedy search with sampling\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use the id of the model you want to use\n",
    "# GPT-2 \"openai-community/gpt2\"\n",
    "# Qwen \"Qwen/Qwen2-0.5B\"\n",
    "# SmolLM \"HuggingFaceTB/SmolLM-135M\"\n",
    "\n",
    "prompt = \"It was a dark and stormy\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "input_ids = tokenizer(prompt).input_ids\n",
    "input_ids\n",
    "\n",
    "for t in input_ids:\n",
    "    print(t, \"\\t:\", tokenizer.decode(t))\n",
    "\n",
    "# We tokenize again but specifying the tokenizer that we want it to\n",
    "# return a PyTorch tensor, which is what the model expects,\n",
    "# rather than a list of integers\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model(input_ids)\n",
    "outputs.logits.shape  # An output for each input token\n",
    "\n",
    "final_logits = model(input_ids).logits[0, -1]  # The last set of logits\n",
    "final_logits.argmax()  # The position of the maximum\n",
    "\n",
    "tokenizer.decode(final_logits.argmax())\n",
    "\n",
    "import torch\n",
    "\n",
    "top10_logits = torch.topk(final_logits, 10)\n",
    "for index in top10_logits.indices:\n",
    "    print(tokenizer.decode(index))\n",
    "\n",
    "top10 = torch.topk(final_logits.softmax(dim=0), 10)\n",
    "for value, index in zip(top10.values, top10.indices):\n",
    "    print(f\"{tokenizer.decode(index):<10} {value.item():.2%}\")\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=20)\n",
    "decoded_text = tokenizer.decode(output_ids[0])\n",
    "\n",
    "print(\"Input IDs\", input_ids[0])\n",
    "print(\"Output IDs\", output_ids)\n",
    "print(f\"Generated text: {decoded_text}\")\n",
    "\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    num_beams=5,\n",
    "    max_new_tokens=30,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(beam_output[0]))\n",
    "\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    num_beams=5,\n",
    "    repetition_penalty=2.0,\n",
    "    max_new_tokens=38,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(beam_output[0]))\n",
    "\n",
    "## Do Sample\n",
    "\n",
    "from transformers import set_seed\n",
    "\n",
    "# Setting the seed ensures we get the same results every time we run this code\n",
    "set_seed(70)\n",
    "\n",
    "sampling_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=34,\n",
    "    top_k=0,  # We'll come back to this parameter\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sampling_output[0]))\n",
    "\n",
    "## Temperature\n",
    "\n",
    "sampling_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.4,\n",
    "    max_new_tokens=40,\n",
    "    top_k=0,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sampling_output[0]))\n",
    "\n",
    "sampling_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.001,\n",
    "    max_new_tokens=40,\n",
    "    top_k=0,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sampling_output[0]))\n",
    "\n",
    "sampling_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=3.0,\n",
    "    max_new_tokens=40,\n",
    "    top_k=0,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sampling_output[0]))\n",
    "\n",
    "## Top-K\n",
    "\n",
    "sampling_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=40,\n",
    "    top_k=5,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sampling_output[0]))\n",
    "\n",
    "## Top - P\n",
    "\n",
    "sampling_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=40,\n",
    "    top_p=0.94,\n",
    "    top_k=0,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sampling_output[0]))\n",
    "\n",
    "## Day 4 assessment\n",
    "\n",
    "from transformers import GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, GPT2LMHeadModel, pipeline, \\\n",
    "                         Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')  # load up a standard gpt2 model\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# set our pad token to be the eos token. This lets gpt know how to fill space\n",
    "\n",
    "# load up our data into a dataset\n",
    "pds_data = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='/content/Hands-On-Large-Language-Models.txt',  # Principles of Data Science - Sinan Ozdemir\n",
    "    block_size=64  # length of each chunk of text to use as a datapoint\n",
    ")\n",
    "\n",
    "pds_data[0], pds_data[0].shape  # inspect the first point\n",
    "\n",
    "print(tokenizer.decode(pds_data[0]))\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    "    # MLM is Masked Language Modelling (for BERT + auto-encoding tasks)\n",
    ")\n",
    "\n",
    "# example of how collator pads data dynamically\n",
    "collator_example = data_collator([tokenizer('I am an input'), tokenizer('So am I')])\n",
    "\n",
    "collator_example\n",
    "\n",
    "collator_example.input_ids  # 50256 is our pad token id\n",
    "\n",
    "tokenizer.pad_token_id\n",
    "\n",
    "collator_example.attention_mask  # Note the 0 in the attention mask where we have a pad token\n",
    "\n",
    "collator_example.labels  # note the -100 to ignore loss calculation for the padded token\n",
    "# Labels are shifted inside the GPT model so we don't need to worry about that\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')  # load up a GPT2 model\n",
    "\n",
    "pretrained_generator = pipeline(  # create a generator with built in params\n",
    "    'text-generation', model=model, tokenizer='gpt2',\n",
    "    config={'max_length': 200, 'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10}\n",
    ")\n",
    "\n",
    "print('----------')\n",
    "for generated_sequence in pretrained_generator('what is agents', num_return_sequences=3):\n",
    "    print(generated_sequence['generated_text'])\n",
    "    print('----------')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_pds\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=32, # batch size for training\n",
    "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=pds_data.examples[:int(len(pds_data.examples)*.8)],\n",
    "    eval_dataset=pds_data.examples[int(len(pds_data.examples)*.8):]\n",
    ")\n",
    "\n",
    "trainer.evaluate()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate()  # loss decrease is slowing down so we are hitting our limit\n",
    "\n",
    "trainer.save_model()\n",
    "\n",
    "loaded_model = GPT2LMHeadModel.from_pretrained('./gpt2_pds')\n",
    "\n",
    "finetuned_generator = pipeline(\n",
    "    'text-generation', model=loaded_model, tokenizer=tokenizer,\n",
    "    config={'max_length': 200, 'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10}\n",
    ")\n",
    "\n",
    "# examples are now sustainably about data\n",
    "print('----------')\n",
    "for generated_sequence in finetuned_generator('what is agents', num_return_sequences=3):\n",
    "    print(generated_sequence['generated_text'])\n",
    "    print('----------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
